\chapter{Analysis}
We will describe some of the aspects of handling student submissions
in this analysis chapter of the report

\section{Data overview}

The system are required to handle  several sensitive data items which must be
protected from adversaries before the online assessment system can be
put into production. The types of data are listed here in order of
decreasing sensitivity.

\begin{enumerate}
  \item The submissions of other students
  \item The code for assessing an assignment
  \item The completed assessments of assignments.
\end{enumerate}

The first two items on the list are close in terms of
sensitivity. We regard the submissions of other students as being the
most sensitive data since they have a larger potential to enable
plagiarism.

Since it would have less impact if an adversary gained unintended access
to the code for assessing an assignment we don't assign it the same
sensitivity. Getting access to the assessment code would only tell you
what kind of output that your assignment should produce in order to
pass. It would not tell you how to make the actual assignment. Thus,
an assignment that is "`tuned"' to produce the output or contain the
phrases that is checked for would not pass subsequent human
inspection. A possible exception to this occurs should this system
ever be used to correct assignments with a single correct answer. In
this case, getting hold of the assessment code equates acquiring the
answers for the assignment. However, we don't expect the system to be
commonly used in this way.

Assessments generated by the system is the least sensitive data being
handled since the, a) do not identify students by name \fxnote{Is it
  always the case that they do not identify students by name?} and b)
TODO

Because of the data that we handle, securing assignments from
unauthorized access is our primary priority.

\section{Assignment lifecycle}
In this section we describe the life cycle phases which an assignment
goes through from submission to the assessment being returned to the
student. The life cycle of a specific assignment depends on several
factors. Particularly if we are dealing with an identified or
anonymous assignment.

\section{Submission constituents}


\section{Executing untrusted code}
If the OnlienTA system are to fulfill its intended role, it is a
necessity that it executes arbitrary code submitted by
students. \fxnote{rewrite}. For obvious reasons, we cannot trust
student submitted code not to perform inappropriate actions, may these
be unintended site-effects of programming errors or deliberate
malicious attempts to escalate privileges.


\subsection{Sandboxing}
We will handle this challenge by executing the student-submitted code
in sandboxes where they have no chance of inducing side-effects on
other parts of the system, gaining access to privileged data or
affecting the assessment of other students assignments.

%Many variants of sandboxing technologies exists and ra

Sandboxing technologies range from fully-flgedged virtual machines to
rule-based execution environments for programs. We will here focus on
containers which is a type of OS-level virtualization which isolates
the sandboxes by enforcing separate namespaces for e.g. networking,
filesystems and system permissions. The Linux kernel provide support
or this kind of sandboxing through a set of features called
\texttt{namespaces}\cite{namespaces}.

There are several well-known "`packaged"' implementations OS-level
virtualization for Linux which is implemented using the namespaces
features. In this section we will comparatively evaluate some of these
and their suitability for our purpose.



\subsubsection{Docker}
Docker is a relatively recent addition to the family of application
containerization tools. Opposed to most other OS-level virtualization
technologies it focuses on application isolation rather
virtualization of an entire operating system. It has popularized a new
way of deploying and executing applications where, instead of
applications being installed on a shared system, applications are
deployed by starting a container which includes the application, its
dependencies and its configuration. \cite{whatisdocker}

Docker comes with a complete set of tools for building and managing
containers. These could be useful in supporting the TA's job of
developing and testing assessment units for assignments.

On the other hand, the codebase of Docker is very large and
contains a lot of features that are superfluous for our use
case. A large codebase also has a higher chance of containing security
critical bugs. The widespread adoption of docker for security critical
applications, however, mitigates this concern somewhat.

Priviledge escalation vulerabilities has historically been more likely
to arise from peripheral feature sets (as seen in the recent QEmu
floppy driver vulnerability \cite{CVE-2015-3456}), rather than the core containment
functionality itself.

\subsubsection{Sandstone}
As part of a related student project, a minimalist and modular
interface to the Linux container APIs were developed. The philosophy
behind this system, named sandstone, is to provide access to
the sandboxing features of Linux as small individual building blocks
which can be composed, providing the program executed with the
desired level of isolation.

The building blocks of Sandstone is implemented as small programs
written in the C language, each of which implements a part of the
Linux container system. The programs are composed through command
chaining. Exemplifiedied, the composition works as follows: Let $a$,
$b$, and $c$ be programs in Sandstone and let $p$ be the program that
we wish to execute in a sandboxed environment. We execute the process
with the command line \texttt{a b c p}. Then $b$ will be constrained
by the isolation provided by $a$, $c$ will be constrained by $a$ and
$b$ and $p$ will be constrained by $a$, $b$ and $c$ and will thus be
executed with the level of isolation that we desired. \cite{onlineta}

There are good reasons to be wary of custom built security
critical utilities. Security is hard to get right and custom built
utilities that never see widespread use will never go through the
public scrutiny offered to more publicly visible tools. A mitigating
feature of Sandstone in this regard is its relative simplicity and the
limited amount of code used in its implementation.

\subsubsection{Conclusions}


The design that we propose later in this report will provide an

%\section{Privileged operations}
%The system needs to perform several operations which cannot be
%performed by a regular user in Linux. In this section, we will li

\section{Design principles}
We have devised a number of principles that our system design will be
based on. These principles aims to simplify the design of our s

\paragraph{Modularity}
We want the components of our system to be modular and loosely
coupled. Large monolithic components tends to foster unmanageable
complexity which makes them harder to maintain and review for security
problems.
%As it will become clear from our following
%principles
Modularity also adds flexibility to our system by enabling certain
components to be swapped.

\paragraph{Clearly defined minimal interfaces}


\paragraph{Minimum permssions}
A common "`best practice"' related to the security aspects of software
development is to assign a minimalized set of permission to a
component required by the work that it needs to perform. Traditional
Unix environments only makes the distinction between a superuser (root)
and a regular user. It's common for applications to be started with
full superuser permissions and then, after performing privileged
actions,resume operations as a unprivileged user.

%This helps
%mitigating the effects of exploitation of security 

The sharp separation between superusers and regular users no longer
exists in model versions of the Linux kernel following the
introduction of the \textit{Capabilities} interface \cite{cap7}. 

Several operations 

\paragraph{Data ownership separation}
The most basic permission management primitive in Unix operating
systems is the concept of a user. We wish to enforce fine grained user
separation across the system such that a given user only have access
to what is absolutely necessary. A common concern of executing
applications inside a container is what happens in the unlikely event
that a process successfully manages to escape the container. In this
case, the attacker would usually end up having the same permissions as
the process executing the container.... TODO

An important advantage of linux capabilities is that they follow a
\textit{process} and not the user. This means that an attacker would
need to exploit, either So even if an attacker manages to
escape our container, he would 

\subsection{Securing special capabilities}
Despite our best efforts, we can never completely eliminate the
possibility that an adversary will be able to take advantage of the
capabilities given to our programs and force them to preform
unintended actions. For instance, gaining arbitrary code execution in
a process with the \texttt{chown} capability would allow one to change
permissions of any file in the system. Since our security model to a
large extent relies on users having access to a minimalized set of
files, we would consider this to be a rather serious security
incident.

Secure Computing (seccomp) \cite{manpage} is a facility provided by
the Linux kernel to provide application sandboxing through the
filtering of syscalls. It allows an application to unidirectionally
transition into a secure state where all of its syscalls are filtered
by specified rules. Building on our example in the previous paragraph,
we can use the seccomp to filter the \texttt{chown} syscall such that
it only allows changing the owner of files to UID within an allowed
range, e.g. $(\text{aaa000})_{36} - (\text(zzz999)_{36})$.

We can use seccomp to build an extra layer of security around our
programs which limits the effects of \fxnote{discuss problem of go spawning multiple threads per
  default somewhere}


\section{Handling user identities}
We will support assessing submissions from two groups of user:
anonymous and authenticated users. Submissions from these groups will
be handled in two different ways, both with regard to what services
are offered to the user, but also how we represent their submissions
internally.

Anonymous submissions are removed after their life cycle has
concluded. Only the assessment of the submission persists until it has
been read by the submitting user.

\fxnote{We should hold off describing our implementation of handling
  usernames through}

\subsubsection{Identified submissions}
For the time being, the scope of OnlineTA is limited to students at
DIKU. Thus, we can take advantage of the standardized usernames
assigned to everyone who is in some way associated with the University
of Copenhagen (KU-usernames), both students and
employees. KU-usernames are matched by the regular expression
\texttt{[a-z]\{3\}[0-9]\{3\}}, with the exception that vowels aren't used
as letters in generated usernames\fxnote{example?}. We can use this
username generation scheme to our advantage since we can interpret the
KU-usernames as base 36 numbers and thus achieve a stateless and
bidirectional mapping between Linux UIDs and KU-usernames, e.g.:
\begin{equation*}
(\text{gfr534})_{36} = (993919360)_{10}
\end{equation*}

Since UIDs in Linux are represented as a 32-bit signed integer, actual
implementations of this scheme needs to take into account that the
(numerically) largest valid KU-username $(zzz999)_{36}$ is larger than
$(2^{31})_{10}$.

\paragraph{Verifying user identity}
Another ongoing student project are designing and implementing a
purpose-built GPG key server which are intended to be implemented into
the OnlineTA system


\section{Interacting with the system}
In this section, we describe modes of interaction with the system for
making submissions and receiving assessments.

Since this system is intended to be used even by freshmen submitting
their first assignment, providing a simple, familiar, universal and
immediately accessible interface for submitting assignments is an
important goal. However, a secondary purpose of this system is, not
just aiding TAs in their grading work, but also to provide students
with continuous feedback while they are solving their
assignments. It's therefore important that we meet students where they
are and provide them with (from their perspective) the most seamless
interaction possible.

The first goal is best achieved through a well-designed web interface
for uploading assignments. The interface should, when
possible\fxnote{Talk about when system overload causes long submission
  times}, provide immediate feedback within the browser. As previously
mentioned, we have based our user identification on verifying that an
assignment has been signed with the GPG-key belonging to the user
claiming to have submitted the assignment. This poses a potential
obstacle when implementing authenticated submissions through the web
interface. One option, is to require students to GPG sign their
assignments prior to submission... Talk about WAYF, browser
certificates, etc...

Many additional submission interfaces has been proposed since the
conception of OnlineTA. The prevailing proposal has been to implement
a command line interface which would allow a student to submit the
current folder or a file for assessment. The Windows equivalent of
this would be a Shell Extension adding tantamount functionality to the
right click menu of files and folders. We are not qualified to provide
equivalents for other platforms such as OSX, but in that particular
case, the command line interface would also be feasible. A tangential
advantage of this submission method is that it could be implemented to
only transmit deltas between repeated submissions and thus
significantly reduce upload time and internet bandwidth required for
submission.

Another submission interface under consideration is Git. We consider
the addition of 


\section{Large Submissions}
Some courses may require large submissions which could saturate
any feasible internet link through only a limited number of concurrent
uploads. We propose that the upload bandwidth requirement can be
reduced by applying a delta transfer algorithm on repeated
submissions.

The rsync algorithm is an abvious candidate for solving this
problem. However, the alogrim assumes that the receiving end has
access to all files. Honoring our porperty of minimal user permissions
the submission receiving process won't have access to the files of the
previous submissions. We therefore propose a variation of the rsync
algorithm built for our purpose. Since we cannot rely on being able to
retrieve the state of the remote files we must be able to keep the
difference between local and remote files locally. TODO




%Auditing system for similarities in assignments which can be used to
%flag suspected plagiarism


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% TeX-command-extra-options: "-enable-write18"
%%% End:
